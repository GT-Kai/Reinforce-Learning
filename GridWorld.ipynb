{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8614db3e",
   "metadata": {},
   "source": [
    "# 1.What is Grid-World?\n",
    "\n",
    "Grid-World is most commontly used discrete(离散的) Markov Decision Process(MDP, 马尔科夫决策链) example environment in RL.Agent is located on a two-dimensional grid. Each grid cell represents a state. The agent moves through actions, receives rewards, and enters a new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42301da9",
   "metadata": {},
   "source": [
    "# 2.Establish the key components of Grid - World (the five elements of MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d20ca",
   "metadata": {},
   "source": [
    "# GridWorld \n",
    "\n",
    "### 1. **动作空间** \n",
    "\n",
    "- 使用元组表示动作 `[(0,-1), (1,0), (0,1), (-1,0), (0,0)]`\n",
    "\n",
    "### 2. **可视化系统** \n",
    "- 基于 matplotlib 的可视化\n",
    "  - 彩色网格显示\n",
    "  - 实时动画展示智能体移动\n",
    "  - 轨迹记录和显示\n",
    "  - 策略箭头可视化\n",
    "  - 状态值数字显示\n",
    "\n",
    "### 3. **坐标系统标准化** \n",
    "- 0-indexed 坐标系统 `(0,0)` 到 `(m-1,n-1)`\n",
    "\n",
    "### 4. **轨迹记录功能**\n",
    "- 自动记录智能体的完整运动轨迹\n",
    "- 支持轨迹可视化\n",
    "- 便于分析和调试\n",
    "\n",
    "### 5. **奖励系统** \n",
    "- 分离的奖励配置\n",
    "  - `reward_target`: 到达目标的奖励\n",
    "  - `reward_forbidden`: 碰撞/禁止区域的惩罚\n",
    "  - `reward_step`: 每步的小惩罚（鼓励最短路径）\n",
    "\n",
    "### 6. **边界处理** \n",
    "- 完整的边界碰撞检测\n",
    "- 禁止区域碰撞处理（智能体停留在原位）\n",
    "- 清晰的奖励反馈\n",
    "\n",
    "### 7. **策略和价值函数可视化** \n",
    "- `add_policy()`: 在网格上显示策略箭头\n",
    "- `add_state_values()`: 显示每个状态的值\n",
    "- 便于调试 RL 算法\n",
    "\n",
    "### 8. **配置文件管理** \n",
    "- 创建 `examples/arguments.py` 统一管理配置\n",
    "\n",
    "### 9. **返回值标准化** \n",
    "- 符合 Gym/Gymnasium API 标准\n",
    "- `reset()` 返回 `(state, info)`\n",
    "- `step()` 返回 `(state, reward, done, info)`\n",
    "\n",
    "### 10. **代码文档完善** \n",
    "- 详细的 docstring\n",
    "- 参数说明\n",
    "- 使用示例\n",
    "\n",
    "## 使用示例\n",
    "\n",
    "```python\n",
    "env = GridWorld(env_size=(3,3), start_state=(0,0),\n",
    "                target_state=(2,2), forbidden_states=[(1,1)])\n",
    "state, info = env.reset()\n",
    "next_state, reward, done, info = env.step((0, -1))\n",
    "env.render()  # 图形化显示\n",
    "env.add_policy(policy_matrix)  # 可视化策略\n",
    "env.add_state_values(values)   # 可视化状态值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170b30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from conf.arguments import args\n",
    "\n",
    "\n",
    "class GridWorld():\n",
    "    \"\"\"\n",
    "    网格世界环境：\n",
    "    \n",
    "    1. 使用元组形式的动作空间 (dx, dy)，更高效\n",
    "    2. 完整的可视化功能（matplotlib）\n",
    "    3. 支持轨迹记录和动画\n",
    "    4. 支持策略可视化和状态值显示\n",
    "    5. 奖励系统\n",
    "    6. 0-indexed 坐标系统，更符合编程习惯\n",
    "    7. 详细的边界和碰撞处理\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 env_size=args.env_size, \n",
    "                 start_state=args.start_state, \n",
    "                 target_state=args.target_state, \n",
    "                 forbidden_states=args.forbidden_states):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            env_size: (rows, cols) 环境大小\n",
    "            start_state: (x, y) 起始状态，0-indexed\n",
    "            target_state: (x, y) 目标状态\n",
    "            forbidden_states: [(x, y), ...] 禁止状态列表\n",
    "        \"\"\"\n",
    "        self.env_size = env_size\n",
    "        self.num_states = env_size[0] * env_size[1]\n",
    "        self.start_state = start_state\n",
    "        self.target_state = target_state\n",
    "        self.forbidden_states = forbidden_states\n",
    "\n",
    "        self.agent_state = start_state\n",
    "        self.action_space = args.action_space          \n",
    "        self.reward_target = args.reward_target\n",
    "        self.reward_forbidden = args.reward_forbidden\n",
    "        self.reward_step = args.reward_step\n",
    "\n",
    "        self.canvas = None\n",
    "        self.animation_interval = args.animation_interval\n",
    "\n",
    "        # 颜色配置\n",
    "        self.color_forbid = (0.9290, 0.6940, 0.125)      # 黄色 - 禁止区域\n",
    "        self.color_target = (0.3010, 0.7450, 0.9330)     # 蓝色 - 目标\n",
    "        self.color_policy = (0.4660, 0.6740, 0.1880)     # 绿色 - 策略\n",
    "        self.color_trajectory = (0, 1, 0)                 # 绿色 - 轨迹\n",
    "        self.color_agent = (0, 0, 1)                      # 蓝色 - 智能体\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置环境到初始状态\"\"\"\n",
    "        self.agent_state = self.start_state\n",
    "        self.traj = [self.agent_state] \n",
    "        return self.agent_state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行一个动作\n",
    "        \n",
    "        参数:\n",
    "            action: (dx, dy) 元组，表示移动方向\n",
    "            \n",
    "        返回:\n",
    "            next_state: 下一个状态\n",
    "            reward: 获得的奖励\n",
    "            done: 是否到达终止状态\n",
    "            info: 额外信息字典\n",
    "        \"\"\"\n",
    "        assert action in self.action_space, \"Invalid action\"\n",
    "\n",
    "        next_state, reward = self._get_next_state_and_reward(self.agent_state, action)\n",
    "        done = self._is_done(next_state)\n",
    "\n",
    "        # 添加轨迹记录（带随机偏移以显示路径）\n",
    "        x_store = next_state[0] + 0.03 * np.random.randn()\n",
    "        y_store = next_state[1] + 0.03 * np.random.randn()\n",
    "        state_store = tuple(np.array((x_store, y_store)) + 0.2 * np.array(action))\n",
    "        state_store_2 = (next_state[0], next_state[1])\n",
    "\n",
    "        self.agent_state = next_state\n",
    "\n",
    "        self.traj.append(state_store)   \n",
    "        self.traj.append(state_store_2)\n",
    "        return self.agent_state, reward, done, {}   \n",
    "    \n",
    "    def _get_next_state_and_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        计算下一个状态和奖励\n",
    "        \n",
    "        处理边界碰撞、禁止区域、目标到达等情况\n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        new_state = tuple(np.array(state) + np.array(action))\n",
    "        \n",
    "        # 边界检测\n",
    "        if y + 1 > self.env_size[1] - 1 and action == (0, 1):    # down\n",
    "            y = self.env_size[1] - 1\n",
    "            reward = self.reward_forbidden  \n",
    "        elif x + 1 > self.env_size[0] - 1 and action == (1, 0):  # right\n",
    "            x = self.env_size[0] - 1\n",
    "            reward = self.reward_forbidden  \n",
    "        elif y - 1 < 0 and action == (0, -1):   # up\n",
    "            y = 0\n",
    "            reward = self.reward_forbidden  \n",
    "        elif x - 1 < 0 and action == (-1, 0):  # left\n",
    "            x = 0\n",
    "            reward = self.reward_forbidden \n",
    "        elif new_state == self.target_state:  # 到达目标\n",
    "            x, y = self.target_state\n",
    "            reward = self.reward_target\n",
    "        elif new_state in self.forbidden_states:  # 进入禁止区域\n",
    "            x, y = state  # 保持在原位\n",
    "            reward = self.reward_forbidden        \n",
    "        else:\n",
    "            x, y = new_state\n",
    "            reward = self.reward_step\n",
    "            \n",
    "        return (x, y), reward\n",
    "        \n",
    "    def _is_done(self, state):\n",
    "        \"\"\"检查是否到达终止状态\"\"\"\n",
    "        return state == self.target_state\n",
    "    \n",
    "    def render(self, animation_interval=args.animation_interval):\n",
    "        \"\"\"\n",
    "        使用 matplotlib 渲染环境\n",
    "        \n",
    "        显示：\n",
    "        - 网格\n",
    "        - 禁止区域（黄色）\n",
    "        - 目标区域（蓝色）\n",
    "        - 智能体位置（蓝色星形）\n",
    "        - 运动轨迹（绿色线条）\n",
    "        \"\"\"\n",
    "        if self.canvas is None:\n",
    "            plt.ion()                             \n",
    "            self.canvas, self.ax = plt.subplots()   \n",
    "            self.ax.set_xlim(-0.5, self.env_size[0] - 0.5)\n",
    "            self.ax.set_ylim(-0.5, self.env_size[1] - 0.5)\n",
    "            self.ax.xaxis.set_ticks(np.arange(-0.5, self.env_size[0], 1))     \n",
    "            self.ax.yaxis.set_ticks(np.arange(-0.5, self.env_size[1], 1))     \n",
    "            self.ax.grid(True, linestyle=\"-\", color=\"gray\", linewidth=\"1\", axis='both')          \n",
    "            self.ax.set_aspect('equal')\n",
    "            self.ax.invert_yaxis()                           \n",
    "            self.ax.xaxis.set_ticks_position('top')           \n",
    "            \n",
    "            # 添加坐标标签\n",
    "            idx_labels_x = [i for i in range(self.env_size[0])]\n",
    "            idx_labels_y = [i for i in range(self.env_size[1])]\n",
    "            for lb in idx_labels_x:\n",
    "                self.ax.text(lb, -0.75, str(lb), size=10, ha='center', va='center', color='black')           \n",
    "            for lb in idx_labels_y:\n",
    "                self.ax.text(-0.75, lb, str(lb), size=10, ha='center', va='center', color='black')\n",
    "            self.ax.tick_params(bottom=False, left=False, right=False, top=False, \n",
    "                              labelbottom=False, labelleft=False, labeltop=False)   \n",
    "\n",
    "            # 绘制目标区域\n",
    "            self.target_rect = patches.Rectangle(\n",
    "                (self.target_state[0]-0.5, self.target_state[1]-0.5), \n",
    "                1, 1, linewidth=1, edgecolor=self.color_target, \n",
    "                facecolor=self.color_target)\n",
    "            self.ax.add_patch(self.target_rect)     \n",
    "\n",
    "            # 绘制禁止区域\n",
    "            for forbidden_state in self.forbidden_states:\n",
    "                rect = patches.Rectangle(\n",
    "                    (forbidden_state[0]-0.5, forbidden_state[1]-0.5), \n",
    "                    1, 1, linewidth=1, edgecolor=self.color_forbid, \n",
    "                    facecolor=self.color_forbid)\n",
    "                self.ax.add_patch(rect)\n",
    "\n",
    "            # 初始化智能体和轨迹\n",
    "            self.agent_star, = self.ax.plot([], [], marker='*', color=self.color_agent, \n",
    "                                           markersize=20, linewidth=0.5) \n",
    "            self.traj_obj, = self.ax.plot([], [], color=self.color_trajectory, linewidth=0.5)\n",
    "\n",
    "        # 更新智能体位置和轨迹\n",
    "        self.agent_star.set_data([self.agent_state[0]], [self.agent_state[1]])       \n",
    "        traj_x, traj_y = zip(*self.traj)         \n",
    "        self.traj_obj.set_data(traj_x, traj_y)\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(animation_interval)\n",
    "        if args.debug:\n",
    "            input('press Enter to continue...')     \n",
    " \n",
    "    def add_policy(self, policy_matrix):\n",
    "        \"\"\"\n",
    "        在网格上可视化策略\n",
    "        \n",
    "        参数:\n",
    "            policy_matrix: 策略矩阵，形状为 (num_states, num_actions)\n",
    "                         每个元素表示在该状态下选择该动作的概率\n",
    "        \"\"\"\n",
    "        for state, state_action_group in enumerate(policy_matrix):    \n",
    "            x = state % self.env_size[0]\n",
    "            y = state // self.env_size[0]\n",
    "            for i, action_probability in enumerate(state_action_group):\n",
    "                if action_probability != 0:\n",
    "                    dx, dy = self.action_space[i]\n",
    "                    if (dx, dy) != (0, 0):\n",
    "                        # 绘制动作箭头\n",
    "                        self.ax.add_patch(patches.FancyArrow(\n",
    "                            x, y, \n",
    "                            dx=(0.1+action_probability/2)*dx, \n",
    "                            dy=(0.1+action_probability/2)*dy, \n",
    "                            color=self.color_policy, \n",
    "                            width=0.001, \n",
    "                            head_width=0.05))\n",
    "                    else:\n",
    "                        # stay 动作用圆圈表示\n",
    "                        self.ax.add_patch(patches.Circle(\n",
    "                            (x, y), radius=0.07, \n",
    "                            facecolor=self.color_policy, \n",
    "                            edgecolor=self.color_policy, \n",
    "                            linewidth=1, fill=False))\n",
    "    \n",
    "    def add_state_values(self, values, precision=1):\n",
    "        \"\"\"\n",
    "        在网格上显示状态值\n",
    "        \n",
    "        参数:\n",
    "            values: 状态值数组\n",
    "            precision: 小数精度\n",
    "        \"\"\"\n",
    "        values = np.round(values, precision)\n",
    "        for i, value in enumerate(values):\n",
    "            x = i % self.env_size[0]\n",
    "            y = i // self.env_size[0]\n",
    "            self.ax.text(x, y, str(value), ha='center', va='center', \n",
    "                        fontsize=10, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931333db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：创建并测试优化后的 GridWorld 环境\n",
    "env = GridWorld(\n",
    "    env_size=(5, 5),\n",
    "    start_state=(0, 0),\n",
    "    target_state=(4, 4),\n",
    "    forbidden_states=[(1, 1), (2, 2), (3, 1)]\n",
    ")\n",
    "\n",
    "# 重置环境\n",
    "state, info = env.reset()\n",
    "print(f\"初始状态: {state}\")\n",
    "\n",
    "# 执行几步\n",
    "actions = [(1, 0), (0, 1), (1, 0), (0, 1)]  # 向右、向下、向右、向下\n",
    "for action in actions:\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(f\"动作: {action}, 状态: {next_state}, 奖励: {reward:.2f}, 完成: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 策略可视化示例\n",
    "env_policy = GridWorld(\n",
    "    env_size=(3, 3),\n",
    "    start_state=(0, 0),\n",
    "    target_state=(2, 2),\n",
    "    forbidden_states=[(1, 1)]\n",
    ")\n",
    "\n",
    "state, _ = env_policy.reset()\n",
    "env_policy.render()\n",
    "\n",
    "# 创建一个简单的策略矩阵 (9个状态，5个动作)\n",
    "# 策略：总是向右或向下移动\n",
    "policy_matrix = np.zeros((9, 5))\n",
    "for i in range(9):\n",
    "    x = i % 3\n",
    "    y = i // 3\n",
    "    if (x, y) == (2, 2):  # 目标状态，选择 stay\n",
    "        policy_matrix[i, 4] = 1.0\n",
    "    elif (x, y) == (1, 1):  # 禁止状态\n",
    "        policy_matrix[i, 4] = 1.0\n",
    "    else:\n",
    "        if x < 2:\n",
    "            policy_matrix[i, 1] = 0.5  # 向右\n",
    "        if y < 2:\n",
    "            policy_matrix[i, 2] = 0.5  # 向下\n",
    "\n",
    "env_policy.add_policy(policy_matrix)\n",
    "plt.title(\"策略可视化示例\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状态值可视化示例\n",
    "env_values = GridWorld(\n",
    "    env_size=(3, 3),\n",
    "    start_state=(0, 0),\n",
    "    target_state=(2, 2),\n",
    "    forbidden_states=[(1, 1)]\n",
    ")\n",
    "\n",
    "state, _ = env_values.reset()\n",
    "env_values.render()\n",
    "\n",
    "# 创建示例状态值（例如：离目标越近值越大）\n",
    "state_values = np.zeros(9)\n",
    "for i in range(9):\n",
    "    x = i % 3\n",
    "    y = i // 3\n",
    "    if (x, y) == (2, 2):  # 目标\n",
    "        state_values[i] = 10.0\n",
    "    elif (x, y) == (1, 1):  # 禁止状态\n",
    "        state_values[i] = -5.0\n",
    "    else:\n",
    "        # 基于曼哈顿距离的简单值函数\n",
    "        dist = abs(2 - x) + abs(2 - y)\n",
    "        state_values[i] = 10.0 - dist * 2\n",
    "\n",
    "env_values.add_state_values(state_values, precision=1)\n",
    "plt.title(\"状态值可视化示例\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a864e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化示例\n",
    "env_vis = GridWorld(\n",
    "    env_size=(5, 5),\n",
    "    start_state=(0, 0),\n",
    "    target_state=(4, 4),\n",
    "    forbidden_states=[(1, 1), (2, 2), (3, 1)]\n",
    ")\n",
    "\n",
    "# 执行一个简单的episode并可视化\n",
    "state, _ = env_vis.reset()\n",
    "env_vis.render()\n",
    "\n",
    "# 简单的向目标移动\n",
    "for _ in range(10):\n",
    "    # 简单策略：交替向右和向下\n",
    "    action = (1, 0) if np.random.rand() > 0.5 else (0, 1)\n",
    "    state, reward, done, _ = env_vis.step(action)\n",
    "    env_vis.render(animation_interval=0.3)\n",
    "    if done:\n",
    "        print(\"到达目标！\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_ai_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
